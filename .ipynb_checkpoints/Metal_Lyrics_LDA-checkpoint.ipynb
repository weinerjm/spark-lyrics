{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import Tokenizer, CountVectorizer, StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName('Metal Lyrics LDA') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from pyspark.sql.functions import monotonically_increasing_id as mid\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "documents = sc.wholeTextFiles('data_test/')\n",
    "df = spark.createDataFrame(documents,['doc_name','doc_text'])\n",
    "\n",
    "# This will return a new DF with all the columns + id\n",
    "df = df.withColumn(\"id\", mid())\n",
    "tokenizer = Tokenizer(inputCol=\"doc_text\", outputCol=\"words\")\n",
    "df = tokenizer.transform(df)\n",
    "\n",
    "stop_words = StopWordsRemover.loadDefaultStopWords('english')\n",
    "stop_words += StopWordsRemover.loadDefaultStopWords('german')\n",
    "stop_words += StopWordsRemover.loadDefaultStopWords('spanish')\n",
    "stop_words += StopWordsRemover.loadDefaultStopWords('french')\n",
    "stop_words += [\"i'm\",' ','','-',\"don't\",\"you're\",\"i'll\",\"can't\",\"it'\",\n",
    "              \"we'll\",\"it's\",\"ne \",\"i've\",\"you'll\",\"let\",\"there's\",\"oh\"]\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\", stopWords=stop_words)\n",
    "removed = remover.transform(df)\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"filtered\", outputCol=\"vectors\")\n",
    "model = cv.fit(removed)\n",
    "df_vec = model.transform(removed)\n",
    "# df_vec.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import DenseVector\n",
    "corpus = df_vec.select(\"id\",\"vectors\").rdd.map(lambda (x, y): [x,DenseVector(y.toArray())]).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cluster the documents into five topics using LDA\n",
    "NUM_TOPICS = 5\n",
    "ldaModel = LDA.train(corpus, k=NUM_TOPICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36415 words in vocabulary\n",
      "\n",
      "Top words for topic 0\n",
      "eyes, life, time, like, back, never, one, go, know, see, wake, blood, hell, way, come, cry, head, could, take, feel\n",
      "\n",
      "Top words for topic 1\n",
      "life, never, light, us, one, time, world, day, come, give, death, left, right, last, blood, black, know, new, way, nothing\n",
      "\n",
      "Top words for topic 2\n",
      "know, see, go, time, like, never, come, love, one, forever, take, world, heart, still, away, me,, make, night, gonna, made\n",
      "\n",
      "Top words for topic 3\n",
      "like, see, time, feel, one, life, never, end, want, take, ready, know, got, way, world, get, make, mind, come, things\n",
      "\n",
      "Top words for topic 4\n",
      "night, see, one, time, know, never, take, dead, like, stand, inside, got, death, life, us, find, end, flesh, feel, break\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_WORDS = 20\n",
    "topics = ldaModel.describeTopics(NUM_WORDS)\n",
    "print \"{} words in vocabulary\".format(len(model.vocabulary))\n",
    "print \"\"\n",
    "\n",
    "for i, t in enumerate(topics):\n",
    "    print \"Top words for topic {}\".format(i)\n",
    "    word_indices, weights = t\n",
    "    result = []\n",
    "    for idx in range(len(word_indices)):\n",
    "        #print \"{} : {}\".format(model.vocabulary[word_indices[idx]].encode('utf-8'), \n",
    "        #                       weights[idx])\n",
    "        result.append(model.vocabulary[word_indices[idx]].encode('utf-8'))\n",
    "    print ', '.join(result)\n",
    "    print \"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
