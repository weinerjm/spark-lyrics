{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA topic modeling using MLLib\n",
    "Using some lyrics that I had downloaded from DarkLyrics.com, try to see what major themes are expressed in song lyrics.\n",
    "\n",
    "Note that this is by song and not by album."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import Tokenizer, CountVectorizer, StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName('Metal Lyrics LDA') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data into a `pyspark.sql` DataFrame.\n",
    "\n",
    "1500 text files containing lyrics are in `data_test/`, randomly sampled from ~200,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id as mid\n",
    "\n",
    "documents = sc.wholeTextFiles('data_test/')\n",
    "df = spark.createDataFrame(documents,['doc_name','doc_text'])\n",
    "\n",
    "# This will return a new DF with all the columns + id\n",
    "df = df.withColumn(\"id\", mid())\n",
    "tokenizer = Tokenizer(inputCol=\"doc_text\", outputCol=\"words\")\n",
    "df = tokenizer.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the lyrics are in many different languages, remove stop words from several languages. Unfortunately, this removes the word \"die\" from English as well as German lyrics. Otherwise though, the topics would roughly be arranged into languages.\n",
    "\n",
    "Use `CountVectorizer` to do the word counting instead of the usual method of applying the standard parallelizable combinations of `.map` and `.reduceByKey()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_words = StopWordsRemover.loadDefaultStopWords('english')\n",
    "stop_words += StopWordsRemover.loadDefaultStopWords('german')\n",
    "stop_words += StopWordsRemover.loadDefaultStopWords('spanish')\n",
    "stop_words += StopWordsRemover.loadDefaultStopWords('french')\n",
    "stop_words += [\"i'm\",' ','','-',\"don't\",\"you're\",\"i'll\",\"can't\",\"it'\",\n",
    "              \"we'll\",\"it's\",\"ne \",\"i've\",\"you'll\",\"let\",\"there's\",\"oh\"]\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\", stopWords=stop_words)\n",
    "removed = remover.transform(df)\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"filtered\", outputCol=\"vectors\")\n",
    "model = cv.fit(removed)\n",
    "df_vec = model.transform(removed)\n",
    "# df_vec.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something that isn't clear in the Python documentation: I was getting an error when passing `(document_key, SparseVector())` tuples to the LDA constructor. In the docs, they refer you to CountVectorizer for performing word counts, but it was throwing an error saying that it expected a Vector type. So I instead convert to `(id, DenseVector())` pairs.\n",
    "\n",
    "I imagine that it's much clearer in Scala!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import DenseVector\n",
    "corpus = df_vec.select(\"id\",\"vectors\").rdd.map(lambda (x, y): [x,DenseVector(y.toArray())]).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cluster the documents into five topics using LDA\n",
    "NUM_TOPICS = 5\n",
    "ldaModel = LDA.train(corpus, k=NUM_TOPICS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the results with the top 20 words for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36415 words in vocabulary\n",
      "\n",
      "Top words for topic 0\n",
      "life, time, one, blood, night, eyes, flesh, like, day, cry, wake, death, soul, heart, world, see, end, us, dead, never\n",
      "\n",
      "Top words for topic 1\n",
      "time, life, never, see, one, us, black, night, away, world, take, fire, know, new, eyes, heart, pain, come, back, mind\n",
      "\n",
      "Top words for topic 2\n",
      "death, itâs, see, time, life, like, fall, na, one, silence, keep, sense, words, away, dead, never, death,, must, frozen, behind\n",
      "\n",
      "Top words for topic 3\n",
      "see, come, one, go, like, eyes, light, time, never, life, could, know, dead, make, dark, mind, forever, god, take, us\n",
      "\n",
      "Top words for topic 4\n",
      "like, know, got, feel, get, see, one, never, take, time, way, come, end, love, world, life, want, we're, every, right\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_WORDS = 20\n",
    "topics = ldaModel.describeTopics(NUM_WORDS)\n",
    "print \"{} words in vocabulary\".format(len(model.vocabulary))\n",
    "print \"\"\n",
    "\n",
    "for i, t in enumerate(topics):\n",
    "    print \"Top words for topic {}\".format(i)\n",
    "    word_indices, weights = t\n",
    "    result = []\n",
    "    for idx in range(len(word_indices)):\n",
    "        #print \"{} : {}\".format(model.vocabulary[word_indices[idx]].encode('utf-8'), \n",
    "        #                       weights[idx])\n",
    "        result.append(model.vocabulary[word_indices[idx]].encode('utf-8'))\n",
    "    print ', '.join(result)\n",
    "    print \"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
